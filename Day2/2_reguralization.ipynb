{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 正則化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression #線形回帰のライブラリ\n",
    "from sklearn.linear_model import Ridge,Lasso,ElasticNet #正則化項付き最小二乗法を行うためのライブラリ\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 正則化とは\n",
    "多次元の回帰モデルを考える場合、出力に大きく影響する説明変数もあれば、ほとんど影響しない変数もあるはずである。ここで、未知データに強いということは、出力への寄与が小さい影響は無視して、出力への影響が大き良い特徴だけで構成された回帰式だということが想像される。つまり、回帰式中の係数$w$に関して、大きな値を持つものがなるべく小さい方が未知データに対して、頑健な回帰式であるということができる。このように大きな値をとる係数を減らすための工夫を正則化とよぶ。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### データの生成\n",
    "参考サイト\n",
    "http://nbviewer.jupyter.org/urls/s3.amazonaws.com/datarobotblog/notebooks/regularized-linear-regression.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return np.sin(2 * np.pi * x)\n",
    "\n",
    "x_plot = np.linspace(0, 1, 100)\n",
    "n_samples = 10\n",
    "X = np.random.uniform(0, 1, size=n_samples)[:, np.newaxis]\n",
    "y = f(X) + np.random.normal(scale=0.3, size=n_samples)[:, np.newaxis]\n",
    "\n",
    "plt.plot(x_plot, f(x_plot), color='green')\n",
    "plt.scatter(X, y, s=10)\n",
    "plt.ylim((-2, 2))\n",
    "plt.xlim((0, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### make_pipeline(PolynomialFeatures(degree), LinearRegression())を用いると、多項式回帰モデルを推定することができる。  \n",
    "#### degreeの違い\n",
    "degree=0:$y=a_0$  \n",
    "degree=1:$y=a_0+a_1x$  \n",
    "degree=3:$y=a_0+a_1x+a_2x^2+a_3x^3$  \n",
    "degree=9:$y=a_0+\\sum_{i=1}^9 {a_ix^i}$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 正則化項なしで係数を推定した場合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_approximation(est, ax, label=None):\n",
    "    \"\"\"Plot the approximation of ``est`` on axis ``ax``. \"\"\"\n",
    "    ax.plot(x_plot, f(x_plot), color='green')\n",
    "    ax.scatter(X, y, s=10)\n",
    "    ax.plot(x_plot, est.predict(x_plot[:, np.newaxis]), color='red', label=label)\n",
    "    ax.set_ylim((-2, 2))\n",
    "    ax.set_xlim((0, 1))\n",
    "    ax.set_ylabel('y')\n",
    "    ax.set_xlabel('x')\n",
    "    ax.legend(loc='upper right')  #, fontsize='small')\n",
    "\n",
    "def plot_coefficients(est, ax, label=None, yscale='log'):\n",
    "    coef = est.steps[-1][1].coef_.ravel()\n",
    "    if yscale == 'log':\n",
    "        ax.semilogy(np.abs(coef), marker='o', label=label)\n",
    "        ax.set_ylim((1e-1, 1e8))\n",
    "    else:\n",
    "        ax.plot(np.abs(coef), marker='o', label=label)\n",
    "    ax.set_ylabel('abs(coefficient)')\n",
    "    ax.set_xlabel('coefficients')\n",
    "    ax.set_xlim((0, 9))\n",
    "    \n",
    "\n",
    "fig, ax_rows = plt.subplots(4, 2, figsize=(8, 10))\n",
    "degrees = [0, 1, 3, 9]#degreeの値を4つ指定する\n",
    "for ax_row, degree in zip(ax_rows, degrees):\n",
    "    ax_left, ax_right = ax_row\n",
    "    est = make_pipeline(PolynomialFeatures(degree), LinearRegression())\n",
    "    est.fit(X, y)\n",
    "    plot_approximation(est, ax_left, label='degree=%d' % degree)\n",
    "    plot_coefficients(est, ax_right,yscale=None)\n",
    "    \n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- degree=0や1の場合、ほとんど学習データを表現することができていない。これをアンダーフィッティング（未学習）を起こしているという。\n",
    "- degree=9になると、各係数の値が非常に大きくなっている。係数の値が大きいと、$x$への寄与率が大きくなるということである。既知のデータへの当てはまりは良くなるが、未知のデータ（テストデータ）への当てはまりがわるくなる。これを、オーバーフィッティング（過学習）を起こしているという。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [演習]\n",
    "- degree=7の場合はどうなりますか？\n",
    "- degree=20の場合はどうなりますか？\n",
    "- データ点数(n_samples)とdegreeの値をいろいろ変えて、結果がどう変わるか確認してみましょう"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 正則化項\n",
    "最小二乗法は、目的関数$E(w)=\\sum_i^n (y_i-y_i')^2$を最小化することが目的であった。正則化項付きの最小二乗法では、  これに正則化項が付いた目的関数を最小化することが目的になる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Ridgeは、係数の値が小さくなるように正則化される。\n",
    "- Lassoは、値を0とする係数が多くなるように正則化される。\n",
    "- ElasticNetは、RidgeとLassoを組み合わせたものであり、両者のバランスによって性質が決まる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 正則化項ありで係数を推定した場合"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax_rows = plt.subplots(4, 2, figsize=(8, 10))\n",
    "degree = 9\n",
    "alphas = [0.0, 1e-8, 1e-5, 1e-1]#alpha(数式ではλ)の値を4つ指定する\n",
    "for alpha, ax_row in zip(alphas, ax_rows):\n",
    "    ax_left, ax_right = ax_row\n",
    "    # ConvergenceWarning が出るときがあるが、グラフが表示されていれば実行は出来ているので気にしなくて良い\n",
    "    est = make_pipeline(PolynomialFeatures(degree), Ridge(alpha=alpha))\n",
    "    est.fit(X, y)\n",
    "    plot_approximation(est, ax_left, label='alpha=%r' % alpha)\n",
    "    plot_coefficients(est, ax_right, label='Ridge(alpha=%r) coefficients' % alpha)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\alpha$を大きくすると、係数の値が小さくなっていき、過学習が抑制されることがわかる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [演習]\n",
    "- alphaの値をいろいろ変えて、結果がどう変わるか確認してみましょう"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax_rows = plt.subplots(4, 2, figsize=(8,10))\n",
    "\n",
    "degree = 9\n",
    "alphas = [1e-20, 1e-3, 1e-2, 1e-1]#alpha(数式ではλ)の値を4つ指定する\n",
    "for alpha, ax_row in zip(alphas, ax_rows):\n",
    "    ax_left, ax_right = ax_row\n",
    "    # ConvergenceWarning が出るときがあるが、グラフが表示されていれば実行は出来ているので気にしなくて良い\n",
    "    est = make_pipeline(PolynomialFeatures(degree), Lasso(alpha=alpha,max_iter=1e7))\n",
    "    est.fit(X, y)\n",
    "    plot_approximation(est, ax_left, label='alpha=%r' % alpha)\n",
    "    plot_coefficients(est, ax_right, label='Lasso(alpha=%r) coefficients' % alpha, yscale=None)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\alpha$を大きくすると、0の係数が増えていき、過学習が抑制されることがわかる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [演習]\n",
    "- alphaの値をいろいろ変えて、結果がどう変わるか確認してみましょう"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ElasticNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax_rows = plt.subplots(4, 2, figsize=(8,10))\n",
    "\n",
    "degree = 20\n",
    "alpha = 1e-4 #正則化全体の強さを決定する\n",
    "l1_ratios = [0, 0.1, 0.5, 1.0] #L1正則化の強さを4つ指定する（L2正則化の強さは1 - l1_ratioで自動的に設定される）\n",
    "for l1_ratio, ax_row in zip(l1_ratios, ax_rows):\n",
    "    ax_left, ax_right = ax_row\n",
    "    # ConvergenceWarning が出るときがあるが、グラフが表示されていれば実行は出来ているので気にしなくて良い\n",
    "    est = make_pipeline(PolynomialFeatures(degree), ElasticNet(alpha=alpha, l1_ratio=l1_ratio, max_iter=1e8))\n",
    "    est.fit(X, y)\n",
    "    plot_approximation(est, ax_left, label='l1_ratio=%r' % l1_ratio)\n",
    "    plot_coefficients(est, ax_right, label='ElasticNet(l1_ratio=%r) coefficients' % l1_ratio, yscale=None)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - `l1_ratio` が小さければL2正則化の効果が強く現れ，逆に大きければL1正則化の効果が強く現れる"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [演習]\n",
    "- alphaやl1_ratioの値をいろいろ変えて、結果がどう変わるか確認してみましょう"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
