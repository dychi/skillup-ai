{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 系列変換モデルで学習する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset import sequence\n",
    "from common.optimizer import Adam\n",
    "from common.trainer import Trainer\n",
    "from common.util import eval_seq2seq\n",
    "from common.seq2seq import Seq2seq # seq2seq\n",
    "from common.attention_seq2seq import AttentionSeq2seq # アテンション付きseq2seq\n",
    "from common.attention_biseq2seq import AttentionBiSeq2seq # エンコーダ側LSTMが双方向になったアテンション付きseq2seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データの読み込み\n",
    "(x_train, t_train), (x_test, t_test) = sequence.load_data('date.txt')\n",
    "char_to_id, id_to_char = sequence.get_vocab()\n",
    "\n",
    "# ハイパーパラメータの設定\n",
    "vocab_size = len(char_to_id)\n",
    "wordvec_size = 16\n",
    "hidden_size = 256\n",
    "batch_size = 256\n",
    "max_epoch = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### モデルの選択\n",
    "モデルを切り替えて、結果を比較してみましょう"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Seq2seq(vocab_size, wordvec_size, hidden_size)\n",
    "model = AttentionSeq2seq(vocab_size, wordvec_size, hidden_size)\n",
    "# model = AttentionBiSeq2seq(vocab_size, wordvec_size, hidden_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 最適化手法の設定\n",
    "optimizer = Adam()\n",
    "\n",
    "# 学習のオブジェクトを生成\n",
    "trainer = Trainer(model, optimizer)\n",
    "\n",
    "# 学習のループ\n",
    "acc_list = []\n",
    "loss_list = []\n",
    "for epoch in range(max_epoch):\n",
    "    \n",
    "    # trainデータで1epoch分の計算\n",
    "    trainer.fit(x_train, t_train, max_epoch=1,\n",
    "                batch_size=batch_size)\n",
    "\n",
    "    # testデータで精度を確認する\n",
    "    correct_num = 0\n",
    "    for i in range(len(x_test)):\n",
    "        question, correct = x_test[[i]], t_test[[i]]\n",
    "        verbose = i < 10\n",
    "        correct_num += eval_seq2seq(model, question, correct,\n",
    "                                    id_to_char, verbose) \n",
    "\n",
    "    # 精度算出\n",
    "    acc = float(correct_num) / len(x_test)\n",
    "    acc_list.append(acc)\n",
    "    print('val acc %.3f%%' % (acc * 100))\n",
    "\n",
    "    # loss算出\n",
    "    loss = model.forward(x_test, t_test)\n",
    "    loss_list.append(loss)\n",
    "    print('val loss %.3f' % (loss))\n",
    "    \n",
    "    # 重み保存\n",
    "    model.save_params()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracyの描画\n",
    "x = np.arange(len(acc_list))\n",
    "plt.plot(x, acc_list, marker='o')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('accuracy')\n",
    "plt.ylim(-0.05, 1.05)\n",
    "plt.show()\n",
    "\n",
    "# Lossの描画\n",
    "plt.plot(x, loss_list, marker='o')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')\n",
    "# plt.ylim(-0.05, 0.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [演習]\n",
    "* Seq2seq、AttentionSeq2seq、AttentionBiSeq2seqのそれぞれの場合を計算し、結果を比較してみましょう。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
